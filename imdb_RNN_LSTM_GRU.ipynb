{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading IMDB dataset...\n",
      "Train size: 25000, Test size: 25000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Step 1: Load IMDB Dataset\n",
    "max_features = 10000  # Limit vocabulary size to 10,000 words\n",
    "max_length = 200  # Limit review length\n",
    "\n",
    "print(\"Downloading IMDB dataset...\")\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(f\"Train size: {len(x_train)}, Test size: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode IMDB reviews for Word2Vec processing and swap the key and value so that the integer value becomes the key\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "#the first 3 characters have special meaning where 0 is for padding, 1 for start of sequence and 2 for unknown word\n",
    "#so we skip first 3 characters and unknown word is given question mark character\n",
    "def decode_review(encoded_review):\n",
    "    return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in encoded_review])\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_review(review):\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)  # Remove punctuation like .,!\n",
    "    review = review.lower()  # Convert to lowercase\n",
    "    words = review.split() #list of words\n",
    "    return \" \".join([word for word in words if word not in stop_words]) #join them back\n",
    "\n",
    "decoded_train = [decode_review(review) for review in x_train]\n",
    "decoded_test = [decode_review(review) for review in x_test]\n",
    "\n",
    "decoded_train = [clean_review(review) for review in decoded_train]\n",
    "decoded_test = [clean_review(review) for review in decoded_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained Word2Vec embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Pre-Trained Word2Vec\n",
    "print(\"Loading pre-trained Word2Vec embeddings...\")\n",
    "word2vec_path = \"C:/AI/LazyNlpDL/machine_learning_examples/Large_files/archive/GoogleNews-vectors-negative300.bin\"  # Path to Google Word2Vec binary file\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create Word2Vec Embedding Matrix (300 Dimension)\n",
    "embedding_dim = 300  \n",
    "\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_features and word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert Reviews to Padded Sequences\n",
    "x_train_padded = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test_padded = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build Models\n",
    "def build_model_rnn():\n",
    "    model = Sequential([\n",
    "        Embedding(max_features, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
    "        tf.keras.layers.SimpleRNN(64, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model_lstm():\n",
    "    model = Sequential([\n",
    "        Embedding(max_features, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model_gru():\n",
    "    model = Sequential([\n",
    "        Embedding(max_features, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
    "        GRU(64, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate Models\n",
    "def evaluate_model(model, x_train, y_train, x_test, y_test, epochs=5):\n",
    "    print(f\"Training {model.name}...\")\n",
    "    model.fit(x_train, y_train, batch_size=32, epochs=epochs, validation_split=0.2, verbose=1)\n",
    "    _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequential_6...\n",
      "Epoch 1/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.4949 - loss: 0.7254 - val_accuracy: 0.4994 - val_loss: 0.6965\n",
      "Epoch 2/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.5032 - loss: 0.7136 - val_accuracy: 0.4958 - val_loss: 0.6975\n",
      "Epoch 3/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.5003 - loss: 0.6997 - val_accuracy: 0.4964 - val_loss: 0.6943\n",
      "Epoch 4/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.5043 - loss: 0.6945 - val_accuracy: 0.5038 - val_loss: 0.6927\n",
      "Epoch 5/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.5053 - loss: 0.6951 - val_accuracy: 0.4998 - val_loss: 0.6936\n",
      "Training sequential_7...\n",
      "Epoch 1/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 32ms/step - accuracy: 0.5017 - loss: 0.6946 - val_accuracy: 0.5216 - val_loss: 0.6913\n",
      "Epoch 2/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 31ms/step - accuracy: 0.5286 - loss: 0.6884 - val_accuracy: 0.5576 - val_loss: 0.6787\n",
      "Epoch 3/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 31ms/step - accuracy: 0.5404 - loss: 0.6863 - val_accuracy: 0.5530 - val_loss: 0.6830\n",
      "Epoch 4/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 32ms/step - accuracy: 0.5233 - loss: 0.6929 - val_accuracy: 0.5220 - val_loss: 0.6908\n",
      "Epoch 5/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5356 - loss: 0.6851 - val_accuracy: 0.5106 - val_loss: 0.6933\n",
      "Training sequential_8...\n",
      "Epoch 1/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 33ms/step - accuracy: 0.5011 - loss: 0.6947 - val_accuracy: 0.5098 - val_loss: 0.6941\n",
      "Epoch 2/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5160 - loss: 0.6907 - val_accuracy: 0.5192 - val_loss: 0.6919\n",
      "Epoch 3/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5321 - loss: 0.6873 - val_accuracy: 0.5396 - val_loss: 0.6860\n",
      "Epoch 4/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.5856 - loss: 0.6660 - val_accuracy: 0.6478 - val_loss: 0.6490\n",
      "Epoch 5/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 33ms/step - accuracy: 0.7132 - loss: 0.5732 - val_accuracy: 0.7532 - val_loss: 0.4985\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate RNN, LSTM, and GRU\n",
    "rnn_model = build_model_rnn()\n",
    "rnn_accuracy = evaluate_model(rnn_model, x_train_padded, y_train, x_test_padded, y_test)\n",
    "\n",
    "lstm_model = build_model_lstm()\n",
    "lstm_accuracy = evaluate_model(lstm_model, x_train_padded, y_train, x_test_padded, y_test)\n",
    "\n",
    "gru_model = build_model_gru()\n",
    "gru_accuracy = evaluate_model(gru_model, x_train_padded, y_train, x_test_padded, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "RNN Accuracy: 48.69%\n",
      "LSTM Accuracy: 51.40%\n",
      "GRU Accuracy: 75.80%\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Report Results\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"RNN Accuracy: {rnn_accuracy * 100:.2f}%\")\n",
    "print(f\"LSTM Accuracy: {lstm_accuracy * 100:.2f}%\")\n",
    "print(f\"GRU Accuracy: {gru_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggemvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
